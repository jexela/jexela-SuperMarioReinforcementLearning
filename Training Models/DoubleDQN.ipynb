{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVVUOQ2ZT-gB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9af2fb-0005-48ed-86f0-7e0d24ffb8c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "\u001b[33mWARNING: Skipping gym-super-mario-bros as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting gym==0.26.2\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym-notices in /usr/local/lib/python3.10/dist-packages (0.0.8)\n",
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (3.1.0)\n",
            "Collecting nes-py>=8.1.4 (from gym-super-mario-bros==7.4.0)\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.66.5)\n",
            "Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym, nes-py\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827619 sha256=0a813f7d6016f81cdc92d101d67250a2ed13483527a22959aa6dd1354e6ec4ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535724 sha256=b055d1bdebe0a6a94921739bfc0aa8990ac13cfa20d9d98f4dfe06c02e1b21b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built gym nes-py\n",
            "Installing collected packages: pyglet, gym, nes-py, gym-super-mario-bros\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2 gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gym_super_mario_bros in /usr/local/lib/python3.10/dist-packages (7.4.0)\n",
            "Requirement already satisfied: nes_py in /usr/local/lib/python3.10/dist-packages (8.2.1)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes_py) (0.26.2)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from nes_py) (1.5.21)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes_py) (4.66.5)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes_py) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, stable-baselines3\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 stable-baselines3-2.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y gym gym-super-mario-bros\n",
        "!pip install gym==0.26.2 gym-notices gym-super-mario-bros==7.4.0\n",
        "!pip install stable-baselines3 gym_super_mario_bros nes_py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importiere grundlegende Bibliotheken\n",
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from gym.wrappers import FrameStack, GrayScaleObservation\n",
        "from gym.spaces import Box\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "# Importiere Bibliotheken für numerische Berechnungen und Deep Learning\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Importiere weitere Hilfsbibliotheken\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "import time\n",
        "\n",
        "# Importiere Bibliotheken für die Interaktion mit Google Colab\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "DyVeSbl8UZd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Wrappers\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, truncated, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, truncated, info\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "# Configure environment\n",
        "#env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
        "#env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "# Environment konfigurieren\n",
        "#env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode=\"none\")\n",
        "#env = gym_super_mario_bros.make('SuperMarioBrosRandomStages-v0', stages=['1-1'], apply_api_compatibility=True, render_mode=\"none\")\n",
        "env = gym_super_mario_bros.make('SuperMarioBrosRandomStages-v0', stages=['1-4'], apply_api_compatibility=True, render_mode=\"none\")\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "\n",
        "# Hyperparameters\n",
        "state_space = env.observation_space.shape  # (4, 84, 84)\n",
        "action_space = env.action_space.n\n",
        "learning_rate = 0.00015\n",
        "gamma = 0.99\n",
        "\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.02\n",
        "epsilon_decay = 0.99997\n",
        "batch_size = 32\n",
        "target_update = 20\n",
        "replayBuffer_size = 300000\n",
        "num_episodes = 14000\n",
        "frame_skip = 1\n",
        "step_count = 0\n",
        "start_learning = 0\n",
        "save_weights = 500\n",
        "\n",
        "# Replay Buffer\n",
        "replayBuffer = deque(maxlen=replayBuffer_size)\n",
        "\n",
        "# Training loop with evaluation\n",
        "training_rewards = []\n",
        "evaluation_rewards = []\n",
        "moving_average_training = []\n",
        "moving_average_evaluation = []\n",
        "min_training_rewards = []\n",
        "max_training_rewards = []\n",
        "min_moving_average_training = []\n",
        "max_moving_average_training = []\n",
        "min_moving_average_evaluation = []\n",
        "max_moving_average_evaluation = []\n",
        "\n",
        "start_episode = 0\n",
        "#evaluation_interval = 100  # Evaluate every 100 episodes\n",
        "#evaluation_episodes = 10  # Number of episodes to evaluate\n",
        "\n",
        "# Define Neural Network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        # Compute the output size after convolutional layers\n",
        "        def conv2d_size_out(size, kernel_size, stride):\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "        conv_h = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[1], 8, 4), 4, 2), 3, 1)\n",
        "        conv_w = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[2], 8, 4), 4, 2), 3, 1)\n",
        "        linear_input_size = conv_w * conv_h * 64\n",
        "\n",
        "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Instantiate Q-networks\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "policy_net = DQN((4, 84, 84), action_space).to(device)  # Adjusted input shape to (4, 84, 84)\n",
        "target_net = DQN((4, 84, 84), action_space).to(device)  # Adjusted input shape to (4, 84, 84)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "#loss_fn = nn.MSELoss()\n",
        "loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "# Preprocess state\n",
        "def preprocess_state(state):\n",
        "    state = np.ascontiguousarray(state)  # Remove negative strides\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    return state\n",
        "\n",
        "# Select action\n",
        "def select_action(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "        return action\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).argmax().item()\n",
        "\n",
        "\n",
        "# Load pretrained networks if necessary\n",
        "policy_net_weights_path = '/content/policy_net_weights.pth'\n",
        "target_net_weights_path = '/content/policy_net_weights.pth'\n",
        "\n",
        "policy_net.load_state_dict(torch.load(policy_net_weights_path))\n",
        "target_net.load_state_dict(torch.load(target_net_weights_path))\n",
        "\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "import os\n",
        "\n",
        "# Initialize log file\n",
        "\n",
        "file_path = \"training_log_double_14.txt\"\n",
        "if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "\n",
        "with open(file_path, 'a') as file:\n",
        "    file.write(\"Episode,Total_Reward,Moving_Average,Episode_Length,Step_Count,Success\\n\")\n",
        "\n",
        "# Only when pretrained\n",
        "# epsilon = 0.02\n",
        "# step_count = 1379678\n",
        "# Episode start 0 or else\n",
        "# start_episode = 23000\n",
        "# num_episodes = 50000\n",
        "\n",
        "\n",
        "for episode in range(start_episode, num_episodes):\n",
        "    # --- Training loop ---\n",
        "    state, _ = env.reset()\n",
        "    state = preprocess_state(state)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    frame_count = 0\n",
        "    episode_length = 0\n",
        "    x_pos_last = 40\n",
        "    time_last = 400\n",
        "    #standing_still_counter = 0\n",
        "\n",
        "    while not done:\n",
        "        action = select_action(state, epsilon)\n",
        "        if episode > start_learning:\n",
        "            epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "\n",
        "        next_state, reward, terminated, truncated, next_info = env.step(action)\n",
        "        next_state = preprocess_state(next_state)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Reward adjustment logic\n",
        "        #if next_info['time'] < time_last:\n",
        "        #    reward += time_penalty\n",
        "        #if next_info['x_pos'] == x_pos_last:\n",
        "        #    reward += position_penalty\n",
        "        #    standing_still_counter += 1\n",
        "        #else:\n",
        "        #    standing_still_counter = 0\n",
        "\n",
        "        #if standing_still_counter >= 4:\n",
        "        #    reward += -10\n",
        "\n",
        "        #x_pos_last = next_info['x_pos']\n",
        "        #time_last = next_info['time']\n",
        "        total_reward += reward\n",
        "\n",
        "        replayBuffer.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        if len(replayBuffer) > batch_size and episode > start_learning:\n",
        "            batch = random.sample(replayBuffer, batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            # Tensor conversion for training\n",
        "            states = torch.cat(states)\n",
        "            actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
        "            rewards = torch.tensor(rewards).to(device)\n",
        "            next_states = torch.cat(next_states)\n",
        "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Compute Q-values and loss\n",
        "            current_q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "            next_state_actions = policy_net(next_states).argmax(dim=1)\n",
        "            next_q_values = target_net(next_states).gather(1, next_state_actions.unsqueeze(1)).squeeze(1)\n",
        "            expected_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "            loss = loss_fn(current_q_values, expected_q_values.detach())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        step_count += 1\n",
        "        episode_length += 1\n",
        "\n",
        "        if done:\n",
        "            # Save training rewards and calculate moving averages\n",
        "            training_rewards.append(total_reward)\n",
        "            moving_average_training.append(np.mean(training_rewards[-100:]))\n",
        "            min_training_rewards.append(np.min(training_rewards[-100:]))\n",
        "            max_training_rewards.append(np.max(training_rewards[-100:]))\n",
        "            min_moving_average_training.append(np.min(moving_average_training[-100:]))\n",
        "            max_moving_average_training.append(np.max(moving_average_training[-100:]))\n",
        "\n",
        "            # Check if Mario completed the level\n",
        "            if next_info['flag_get']:\n",
        "                success = 1  # Mario successfully completed the level\n",
        "            else:\n",
        "                success = 0  # Mario did not complete the level\n",
        "\n",
        "            # Output information about the episode\n",
        "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "            print(f\"Replay Buffer Memory: {len(replayBuffer)}, Episode Length: {episode_length}, Step Count: {step_count}\")\n",
        "            print(f\"Moving Average (Training): {moving_average_training[-1]}, Success: {success}\")\n",
        "\n",
        "            # Write episode data to log file\n",
        "            with open(file_path, 'a') as file:\n",
        "                file.write(f\"{episode},{total_reward},{moving_average_training[-1]},{episode_length},{step_count},{success}\\n\")\n",
        "\n",
        "\n",
        "    if step_count % 7500 == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "      # --- Save model weights ---\n",
        "    if episode % save_weights == 0:\n",
        "        policy_weight_filename = f'policy_net_weights_{episode}.pth'\n",
        "        target_weight_filename = f'target_net_weights_{episode}.pth'\n",
        "        torch.save(policy_net.state_dict(), policy_weight_filename)\n",
        "        torch.save(target_net.state_dict(), target_weight_filename)\n",
        "\n",
        "# Save weights after training\n",
        "torch.save(policy_net.state_dict(), 'policy_net_weights.pth')\n",
        "torch.save(target_net.state_dict(), 'target_net_weights.pth')\n",
        "\n",
        "files.download('policy_net_weights.pth')\n",
        "files.download('target_net_weights.pth')\n",
        "\n",
        "# Save rewards and moving averages to files for later analysis\n",
        "np.save('training_rewards.npy', training_rewards)\n",
        "#np.save('evaluation_rewards.npy', evaluation_rewards)\n",
        "np.save('moving_average_training.npy', moving_average_training)\n",
        "#np.save('moving_average_evaluation.npy', moving_average_evaluation)\n",
        "\n",
        "files.download('training_rewards.npy')\n",
        "files.download('moving_average_training.npy')\n",
        "\n",
        "# Plotting Moving Averages with min and max\n",
        "episodes = range(len(moving_average_training))\n",
        "\n",
        "# Plot the result\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot smoothed Training Moving Average with sliding min and max\n",
        "plt.plot(episodes, moving_average_training, 'b-', label='Avg (Last 100 Episodes)', linewidth=2)\n",
        "plt.plot(episodes, min_moving_average_training, 'r-', linewidth=1.5, label='Min (Last 100 episodes)')\n",
        "plt.plot(episodes, max_moving_average_training, 'g-', linewidth=1.5, label='Max (Last 100 episodes)')\n",
        "\n",
        "# Fill the area between min and max with lighter transparency\n",
        "plt.fill_between(episodes, min_moving_average_training, max_moving_average_training, facecolor='blue', alpha=0.07)\n",
        "\n",
        "# Titles and labels\n",
        "plt.title('Double DQN Moving Average Rewards (Training)')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Reward Value')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "LTmateehVdh3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}